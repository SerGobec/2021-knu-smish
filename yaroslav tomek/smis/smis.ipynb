{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "import re\n",
    "import whois\n",
    "from datetime import date, datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from urllib.parse import urlparse\n",
    "import urllib.request\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size: 1601\n",
      "Legit messages: 1119\n",
      "Spam messages: 161\n",
      "Smishing messages: 321\n"
     ]
    }
   ],
   "source": [
    "# location of dataset\n",
    "os.chdir(r\"C:\\Users\\User\\Documents\\Git\\yaroslav tomek\\data\")\n",
    "\n",
    "# names to label-column and text-column\n",
    "COLUMN_LABEL = \"label\"\n",
    "COLUMN_TEXT = \"text\"\n",
    "\n",
    "# these are labels that indicate the type of message.\n",
    "LABEL_LEGIT = 'LEGI'\n",
    "LABEL_SPAM = 'SPAM'\n",
    "LABEL_SMISHING = 'SMIS'\n",
    "\n",
    "dataset = pd.read_csv('dataset.txt', sep='\\t', names=[COLUMN_LABEL, COLUMN_TEXT], header=None)\n",
    "print('Total size:', dataset.shape[0])\n",
    "print('Legit messages:', dataset[dataset[COLUMN_LABEL] == LABEL_LEGIT].shape[0])\n",
    "print('Spam messages:', dataset[dataset[COLUMN_LABEL] == LABEL_SPAM].shape[0])\n",
    "print('Smishing messages:', dataset[dataset[COLUMN_LABEL] == LABEL_SMISHING].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#перевірити дату реєстарції домену\n",
    "def check_date(url):\n",
    "    domain_name = urlparse(url).netloc\n",
    "    #print('domain:', domain_name)\n",
    "    whois_info = whois.whois(domain_name)\n",
    "    #print(whois_info.creation_date)\n",
    "    d2 = date.today()\n",
    "    try:\n",
    "        d1 = date(whois_info.creation_date.year,whois_info.creation_date.month,whois_info.creation_date.day)\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            d1 = date(whois_info.creation_date[0].year,whois_info.creation_date[0].month,whois_info.creation_date[0].day)\n",
    "        except TypeError:\n",
    "            return 1\n",
    "    except TypeError:\n",
    "        return 1\n",
    "    result = abs(d2-d1).days\n",
    "    #print ('{} days between {} and {}'.format(result, d1, d2))\n",
    "    if result <186:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#print(check_date(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#знайти посилання в смс\n",
    "def find(string):\n",
    "    regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    url = re.findall(regex, string)\n",
    "    return url[0][0]\n",
    "#print(find(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#перевірити чи є form в сарс коді сайту\n",
    "def check_form(url):\n",
    "    user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
    "    headers = {'User-Agent': user_agent, }\n",
    "    request = urllib.request.Request(url, None, headers)  # The assembled request\n",
    "    try:\n",
    "        response = urllib.request.urlopen(request)\n",
    "    except urllib.error.HTTPError:\n",
    "        return 1\n",
    "    except urllib.error.URLError:\n",
    "        return 0\n",
    "    except ConnectionResetError:\n",
    "        return 1\n",
    "    data = response.read()  # The data u need\n",
    "\n",
    "    if '<form ' in str(data):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перетворимо смс у вектори, написавши власні features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages2vectors(messages, size):\n",
    "    features = np.zeros((size, 15))\n",
    "    n=0 #індекс меседжу\n",
    "    greetings = ['hey ', 'Hey','hi ' ,'Hi ', 'Hello', 'hello', 'Good '] #вітання в смс\n",
    "    feelings = ['annoy', 'furious ', 'hate', 'upset', 'disgusted', 'shy', 'uncertain', 'frustrated', 'scare',\n",
    "                'nervous', ':)', ':('] #емоції в смс\n",
    "    symbols = ['+', '-', '%', '@', '^', '/', '$', '<', '>', '{', '[', ']', '|', '}', '#', '*'] #підозрілі символи\n",
    "    selfanswers = ['click', 'follow', 'register ', 'find', 'decline', 'visit', 'go ', 'call ', 'reply', 'send', 'claim',\n",
    "                   'regist'] #накази в смс\n",
    "    smissymb = ['$', '£' , '€', 'UAH'] #знаки грошей\n",
    "    smiskeys = ['award', 'congratulations', 'winner', 'alert', 'claim', 'activate', 'verify', 'attempts', 'gift', 'voucher',\n",
    "                'blocked', 'suspend', 'unlock', 'won', 'prize', 'subscribe', 'activity', 'update', 'coupon', 'refund', 'free',\n",
    "                'ALERT', 'Alert', 'Win', 'Activate', 'Verify', 'GIFT', 'Gift', 'PRIZE', 'Prize', 'FREE', 'Free', 'WON',\n",
    "                'BLOCKED', 'UNLOCK', 'CLAIM', 'Claim', 'SUSPEND', 'UPDATE', 'Update', 'attention', 'Attention',\n",
    "                'ATTENTION', 'WIN', 'locked', 'validat', 'restore', 'Restore', 'RESTORE', 'verify', 'link', 'frozen',\n",
    "                'freeze', 'unfreeze', 'FROZEN', 'locked', 'LOCKED', 'accept', 'ACCEPT', 'money', 'congrats', 'Congrats',\n",
    "                'activity', 'personal', 'Personal', 'PERSONAL'] #ключові слова для смішингу\n",
    "    for vector in features:\n",
    "        # перша та друга фіча за замовчуванням 1, а не 0, бо вони стосуються legi sms\n",
    "        vector[0]=1\n",
    "        vector[1]=1\n",
    "        for gre in greetings:\n",
    "            #якщо наявне привітання, то перша фіча 0, тобто легальна\n",
    "            if gre in messages[n]:\n",
    "                vector[0]=0\n",
    "        for fee in feelings:\n",
    "            #якщо наявна емоція, то друга фіча 0, тобто легальна\n",
    "            if fee in messages[n]:\n",
    "                vector[1]=0\n",
    "        #якщо є лінк в смс, то третя фіча 1, тобто смішинг\n",
    "        if 'http' in messages[n]:\n",
    "            vector[2]=1\n",
    "        #якщо є мат. символи в смс, то четверта фіча 1\n",
    "        for sy in symbols:\n",
    "            if sy in messages[n]:\n",
    "                vector[3]=1\n",
    "        #якщо довжина смс більша 140, то п'ята фіча 1\n",
    "        if len(messages[n])>140:\n",
    "            vector[4]=1\n",
    "        #якщо є селф-ансерс, то шоста фіча 1\n",
    "        for sel in selfanswers:\n",
    "            if sel in messages[n]:\n",
    "                vector[5]=1\n",
    "        #якщо є смішинговй символ, то сьма фіча 1\n",
    "        for smsy in smissymb:\n",
    "            if smsy in messages[n]:\n",
    "                vector[6]=1\n",
    "        #якщо є ключове смішингове слово, то восьма фіча 1\n",
    "        for smkey in smiskeys:\n",
    "            if smkey in messages[n]:\n",
    "                vector[7]=1\n",
    "        #якщо є телефонний номер, то дев'ята фіча 1\n",
    "        if re.findall(r'\\d{7}', messages[n])!=[]:\n",
    "            vector[8]=1\n",
    "        #якщо є email, то десята фіча 1\n",
    "        if re.findall(r'.@\\w{2,6}\\w{2,3}', messages[n])!=[]:\n",
    "            vector[9]=1\n",
    "        #якщо є посилання та домен молодше 6 місяців, то одинацията фіча 1\n",
    "        if vector[2]==1:\n",
    "            vector[10]=check_date(find(messages[n]))\n",
    "            #якщо більше двох слешів в лінку, то дванадцята фіча 1\n",
    "            if len(re.findall('/', find(messages[n])))>=4:\n",
    "                vector[11]=1\n",
    "            #якщо є тире в лінку, то тринадцята фіча 1\n",
    "            if '-' in find(messages[n]):\n",
    "                vector[12]=1\n",
    "            #якщо є форма в сарс коді сайту, то чотирнадцята фіча 1\n",
    "            #print(find(messages[n]))\n",
    "            vector[13]=check_form(find(messages[n]))\n",
    "            #якщо більше двох крапок в лінку, то п'ятнадцята фіча 1\n",
    "            if len(re.findall('\\.', find(messages[n])))>2:\n",
    "                vector[14]=1\n",
    "        else:\n",
    "            vector[10]=0\n",
    "\n",
    "        #print(vector, n)\n",
    "        n+=1\n",
    "\n",
    "    return features\n",
    "\n",
    "#print(dataset[COLUMN_TEXT])\n",
    "#features = messages2vectors(dataset[COLUMN_TEXT], dataset.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перетворимо лейбли у нулі та одиниці"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels(labels_raw):\n",
    "    '''\n",
    "    Transforms labels into numerical values;\n",
    "    Parameters:\n",
    "        labels_raw    -   array of text-labels;\n",
    "    Returns:\n",
    "        features    -   array of numerical labels;\n",
    "    '''\n",
    "\n",
    "    labels = labels_raw.replace('LEGI', 0)\n",
    "    labels = labels.replace('SPAM', 0)\n",
    "    labels = labels.replace('SMIS', 1)\n",
    "\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error trying to connect to socket: closing socket\n",
      "Error trying to connect to socket: closing socket\n",
      "Error trying to connect to socket: closing socket\n",
      "Error trying to connect to socket: closing socket\n",
      "Error trying to connect to socket: closing socket\n",
      "Error trying to connect to socket: closing socket\n",
      "Error trying to connect to socket: closing socket\n",
      "Error trying to connect to socket: closing socket\n",
      "Error trying to connect to socket: closing socket\n",
      "Error trying to connect to socket: closing socket\n",
      "Error trying to connect to socket: closing socket\n",
      "Error trying to connect to socket: closing socket\n",
      "Error trying to connect to socket: closing socket\n",
      "Error trying to connect to socket: closing socket\n",
      "Error trying to connect to socket: closing socket\n",
      "Error trying to connect to socket: closing socket\n",
      "Error trying to connect to socket: closing socket\n",
      "Error trying to connect to socket: closing socket\n",
      "Error trying to connect to socket: closing socket\n",
      "Error trying to connect to socket: closing socket\n",
      "Error trying to connect to socket: closing socket\n",
      "Error trying to connect to socket: closing socket\n",
      "Error trying to connect to socket: closing socket\n",
      "Error trying to connect to socket: closing socket\n",
      "(1601, 15)\n",
      "(1601,)\n"
     ]
    }
   ],
   "source": [
    "features = messages2vectors(dataset[COLUMN_TEXT], dataset.shape[0])\n",
    "labels = convert_labels(dataset[COLUMN_LABEL])\n",
    "\n",
    "print(features.shape)\n",
    "print(labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(features, labels, ratio=0.7):\n",
    "    '''\n",
    "    Splits dataset into train/test parts using given ratio;\n",
    "    Parameters:\n",
    "        data    -   array of features;\n",
    "        labels  -   array of corresponding labels;\n",
    "        ratio   -   train/test size ratio;\n",
    "    Returns:\n",
    "        train_data      -   array of training features;\n",
    "        train_labels    -   array of training labels;\n",
    "        test_data       -   array of testing features;\n",
    "        test_labels     -   array of testing labels;\n",
    "    '''\n",
    "\n",
    "    positive_data = features[labels == 1]  # all spam features\n",
    "    negative_data = features[labels == 0]  # all legit features\n",
    "\n",
    "    # We shuffle arrays to get random samples later\n",
    "    random_indecies_positive = np.arange(positive_data.shape[0])\n",
    "    np.random.shuffle(random_indecies_positive)\n",
    "    random_indecies_negative = np.arange(negative_data.shape[0])\n",
    "    np.random.shuffle(random_indecies_negative)\n",
    "\n",
    "    n_positive_train = int(positive_data.shape[0] * ratio)\n",
    "    n_negative_train = int(negative_data.shape[0] * ratio)\n",
    "\n",
    "    # Training data are all indecies in 'ratio' part of shuffled indecies\n",
    "    train_data = np.concatenate([positive_data[random_indecies_positive[:n_positive_train]],\n",
    "                                 negative_data[random_indecies_negative[:n_negative_train]]])\n",
    "\n",
    "    train_labels = np.asarray([1] * n_positive_train + [0] * n_negative_train)\n",
    "\n",
    "    # Testing data are all indecies that remain\n",
    "    test_data = np.concatenate([positive_data[random_indecies_positive[n_positive_train:]],\n",
    "                                negative_data[random_indecies_negative[n_negative_train:]]])\n",
    "\n",
    "    test_labels = np.asarray(\n",
    "        [1] * (positive_data.shape[0] - n_positive_train) + [0] * (negative_data.shape[0] - n_negative_train))\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "#print(split_data(features,labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Порахуємо FAR та FRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(labels, predictions):\n",
    "    '''\n",
    "    Computes metrics;\n",
    "    Parameters:\n",
    "        labels    -   array of labels;\n",
    "        predictions  -   array of predictions;\n",
    "    Returns:\n",
    "        FAR -   False Acceptance Rate;\n",
    "        FRR -   False Rejection Rate;\n",
    "    '''\n",
    "    #confusion matrix\n",
    "    tp, fn, fp, tn = confusion_matrix(labels, predictions).ravel()\n",
    "    print(pd.DataFrame(confusion_matrix(labels, predictions),\n",
    "                       columns=['Predicted Spam', \"Predicted Legi\"], index=['Actual Spam', 'Actual Legi']))\n",
    "    '''\n",
    "    print(f'\\nTrue Positives: {tp}')\n",
    "    print(f'False Positives: {fp}')\n",
    "    print(f'True Negatives: {tn}')\n",
    "    print(f'False Negatives: {fn}')\n",
    "    '''\n",
    "    FAR = fn/(fn+tp)\n",
    "    FRR = fp/(fp+tn)\n",
    "\n",
    "    #print('metrics', FAR, FRR)\n",
    "    return FAR, FRR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(classifierType, hyperparameters, features, labels):\n",
    "    '''\n",
    "    Splits dataset into train/test parts using given ratio;\n",
    "    Parameters:\n",
    "        classifierType      -   type of ML algorithm to use;\n",
    "        hyperparameters     -   dictionary of model's parameters;\n",
    "        features            -   array of features;\n",
    "        labels              -   array of labels\n",
    "    Returns:\n",
    "        trainFAR    -   False Acceptance Rate for train dataset;\n",
    "        trainFRR    -   False Rejection Rate for train dataset;\n",
    "        testFAR     -   False Acceptance Rate for test dataset;\n",
    "        testFRR    -   False Rejection Rate for test dataset;\n",
    "    '''\n",
    "\n",
    "    model = classifierType(**hyperparameters)\n",
    "\n",
    "    # Split data\n",
    "    train_data, train_labels, test_data, test_labels = split_data(features, labels)\n",
    "\n",
    "    #print('Train set shape:', train_data.shape)\n",
    "    #print('Train labels shape:', train_labels.shape)\n",
    "    #print('Test set shape:', test_data.shape)\n",
    "    #print('Test labels shape:', test_labels.shape)\n",
    "\n",
    "    # Fit your model\n",
    "    model=model.fit(train_data, train_labels)\n",
    "\n",
    "    # Make predictions for training dataset\n",
    "    print(\"---TRAINING---\")\n",
    "    prediction_train=model.predict(train_data)\n",
    "\n",
    "    # Compute train FAR/FRR\n",
    "    trainFAR, trainFRR = get_metrics(train_labels, prediction_train)\n",
    "\n",
    "    # Make predictions for testing dataset\n",
    "    predictions_test = model.predict(test_data)\n",
    "\n",
    "    # Compute test FAR/FRR\n",
    "    print(\"---TESTING---\")\n",
    "    testFAR, testFRR = get_metrics(test_labels, predictions_test)\n",
    "\n",
    "    return trainFAR, trainFRR, testFAR, testFRR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--RANDOM FOREST CLASSIFIER--\n",
      "---TRAINING---\n",
      "             Predicted Spam  Predicted Legi\n",
      "Actual Spam             881              15\n",
      "Actual Legi              37             187\n",
      "---TESTING---\n",
      "             Predicted Spam  Predicted Legi\n",
      "Actual Spam             374              10\n",
      "Actual Legi              16              81\n",
      "(0.016741071428571428, 0.16517857142857142, 0.026041666666666668, 0.16494845360824742)\n"
     ]
    }
   ],
   "source": [
    "classifierType1 = sklearn.ensemble.RandomForestClassifier\n",
    "hyperparameters1 = {'n_estimators' :300,\n",
    "                'criterion' : 'gini',\n",
    "                'max_depth': 8,\n",
    "                'min_samples_split' : 4,\n",
    "                'min_samples_leaf': 1,\n",
    "                'min_weight_fraction_leaf': 0.0,\n",
    "                'max_features': 'log2',\n",
    "                'max_leaf_nodes': None,\n",
    "                'min_impurity_decrease': 0}\n",
    "print(\"--RANDOM FOREST CLASSIFIER--\")\n",
    "print(evaluate(classifierType1, hyperparameters1, features, labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Native Bayes Bernoulli "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--NATIVE BAYES BERNOULLI--\n",
      "---TRAINING---\n",
      "             Predicted Spam  Predicted Legi\n",
      "Actual Spam             793             103\n",
      "Actual Legi              35             189\n",
      "---TESTING---\n",
      "             Predicted Spam  Predicted Legi\n",
      "Actual Spam             338              46\n",
      "Actual Legi              12              85\n",
      "(0.11495535714285714, 0.15625, 0.11979166666666667, 0.12371134020618557)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n"
     ]
    }
   ],
   "source": [
    "classifierType2 = sklearn.naive_bayes.BernoulliNB\n",
    "hyperparameters2 = {'alpha':0,\n",
    "                'binarize':None,\n",
    "                'fit_prior':False,\n",
    "                'class_prior':None,\n",
    "                'fit_prior': False}\n",
    "print(\"--NATIVE BAYES BERNOULLI--\")\n",
    "print(evaluate(classifierType2, hyperparameters2, features, labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Native Bayes Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--NATIVE BAYES CATEGORICAL--\n",
      "---TRAINING---\n",
      "             Predicted Spam  Predicted Legi\n",
      "Actual Spam             790             106\n",
      "Actual Legi              39             185\n",
      "---TESTING---\n",
      "             Predicted Spam  Predicted Legi\n",
      "Actual Spam             342              42\n",
      "Actual Legi               9              88\n",
      "(0.11830357142857142, 0.17410714285714285, 0.109375, 0.09278350515463918)\n"
     ]
    }
   ],
   "source": [
    "classifierType3 = sklearn.naive_bayes.CategoricalNB\n",
    "hyperparameters3 = {'fit_prior': False,\n",
    "                    'alpha': 0.9,\n",
    "                    #'min_categories': None\n",
    "                     }\n",
    "print(\"--NATIVE BAYES CATEGORICAL--\")\n",
    "print(evaluate(classifierType3, hyperparameters3, features, labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
