{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total size: 1540\nLegit messages: 1050\nSpam messages: 190\nSmishing messages: 300\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"data.txt\"\n",
    "COLUMN_LABEL = \"class\"\n",
    "COLUMN_TEXT = \"context\"\n",
    "\n",
    "LABEL_LEGIT = 'LEGI'\n",
    "LABEL_SPAM = 'SPAM'\n",
    "LABEL_SMISHING = 'SMIS'\n",
    "\n",
    "dataset = pd.read_csv(DATA_PATH, sep='\\t', names=[COLUMN_LABEL, COLUMN_TEXT], header=None)\n",
    "print('Total size:', dataset.shape[0])\n",
    "print('Legit messages:', dataset[dataset[COLUMN_LABEL] == LABEL_LEGIT].shape[0])\n",
    "print('Spam messages:', dataset[dataset[COLUMN_LABEL] == LABEL_SPAM].shape[0])\n",
    "print('Smishing messages:', dataset[dataset[COLUMN_LABEL] == LABEL_SMISHING].shape[0])\n",
    "\n",
    "dataset = dataset[((dataset[COLUMN_LABEL] == LABEL_LEGIT) | (dataset[COLUMN_LABEL] == LABEL_SMISHING))]\n",
    "\n",
    "def messages2vectors(messages):\n",
    "\n",
    "    elmo = hub.Module(\"https://tfhub.dev/google/elmo/1\")\n",
    "    features = np.zeros((0, 1024))\n",
    "    n = 100\n",
    "    l = int(len(messages) / n) if len(messages) % n == 0 else int(len(messages) / n) + 1\n",
    "\n",
    "    for i in range(l):\n",
    "\n",
    "        if (i + 1) * n < len(messages):\n",
    "            right = (i + 1) * n\n",
    "            embedds = elmo(messages[int(i * n) : right], signature=\"default\", as_dict=True)[\"default\"] \n",
    "        else:\n",
    "            embedds = elmo(messages[:len(messages) - int(i * n)], signature=\"default\", as_dict=True)[\"default\"] \n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            embedds = sess.run(embedds)\n",
    "            features = np.concatenate([features, embedds])\n",
    "\n",
    "    return features\n",
    "\n",
    "def convert_labels(labels_raw):\n",
    "\n",
    "    labels = np.array([(0 if i==\"LEGI\" else 1) for i in labels_raw ])\n",
    "    return labels\n",
    "\n",
    "labels = convert_labels(dataset[COLUMN_LABEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "(1350, 1024)\n",
      "(1350,)\n"
     ]
    }
   ],
   "source": [
    "features = messages2vectors(dataset[COLUMN_TEXT])\n",
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(features, labels, ratio=0.7):\n",
    "\n",
    "\n",
    "    positive_data = features[labels == 1] \n",
    "    negative_data = features[labels == 0] \n",
    "\n",
    "   \n",
    "    random_indecies_positive = np.arange(positive_data.shape[0])\n",
    "    np.random.shuffle(random_indecies_positive)\n",
    "    random_indecies_negative = np.arange(negative_data.shape[0])\n",
    "    np.random.shuffle(random_indecies_negative)\n",
    "\n",
    "    n_positive_train = int(positive_data.shape[0] * ratio)\n",
    "    n_negative_train = int(negative_data.shape[0] * ratio)\n",
    "\n",
    "    train_data = np.concatenate([positive_data[random_indecies_positive[:n_positive_train]], \n",
    "                                negative_data[random_indecies_negative[:n_negative_train]]])\n",
    "    \n",
    "    train_labels = np.asarray([1] * n_positive_train + [0] * n_negative_train)\n",
    "\n",
    "    test_data = np.concatenate([positive_data[random_indecies_positive[n_positive_train:]], \n",
    "                                negative_data[random_indecies_negative[n_negative_train:]]])\n",
    "\n",
    "    test_labels = np.asarray([1] * (positive_data.shape[0]  - n_positive_train) + [0] * (negative_data.shape[0] - n_negative_train))\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "def get_metrics(labels, predictions):\n",
    "\n",
    "    cf = confusion_matrix(labels, predictions)\n",
    "    FAR = cf[0][1]/(cf[0][1] + cf[0][0])\n",
    "    FRR = cf[1][0]/(cf[1][0] + cf[1][1])\n",
    "    return FAR*100, FRR*100\n",
    "\n",
    "\n",
    "classifierType = [RandomForestClassifier, MultinomialNB,SVC]\n",
    "\n",
    "def evaluate(classifierType, hyperparameters, features, labels,hp , cl = \"not NB\"): \n",
    "\n",
    "  \n",
    "    train_data, train_labels, test_data, test_labels = split_data(features, labels, ratio=0.7)\n",
    "\n",
    "    print('Train set shape:', train_data.shape)\n",
    "    print('Train labels shape:', train_labels.shape)\n",
    "    print('Test set shape:', test_data.shape)\n",
    "    print('Test labels shape:', test_labels.shape)\n",
    "    \n",
    "    model  = GridSearchCV(classifierType(**hp),hyperparameters, n_jobs = -1,refit = \"presicion_score\")\n",
    "   \n",
    "    clf  = model.fit(train_data,train_labels)\n",
    "   \n",
    "    predictions_train = clf.predict(train_data)\n",
    "    \n",
    "    trainFAR, trainFRR = get_metrics(train_labels, predictions_train)\n",
    "\n",
    "    predictions_test = clf.predict(test_data)\n",
    "\n",
    "    testFAR, testFRR = get_metrics(test_labels,predictions_test)\n",
    "    print(\"\\tbest params are \",clf.best_params_)\n",
    "    return trainFAR, trainFRR, testFAR, testFRR\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   RFC :\n",
      "Train set shape: (945, 1024)\n",
      "Train labels shape: (945,)\n",
      "Test set shape: (405, 1024)\n",
      "Test labels shape: (405,)\n",
      "\tbest params are  {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'n_estimators': 85}\n",
      "Train:\n",
      "\tFAR: 0.27210884353741494\n",
      "\tFRR: 10.952380952380953\n",
      "Test:\n",
      "\tFAR: 4.761904761904762\n",
      "\tFRR: 18.88888888888889\n"
     ]
    }
   ],
   "source": [
    "print(\"   RFC :\")\n",
    "hp = {'n_estimators' : 70,\n",
    "                'criterion' : 'gini',\n",
    "                'max_depth' : None,\n",
    "                'min_samples_split' : 2,\n",
    "                'n_jobs' :-1}\n",
    "\n",
    "\n",
    "\n",
    "hyperparameters = {'n_estimators' : list(range(75,200,5)),\n",
    "                'criterion' : ['gini'],\n",
    "                'max_depth' : [None],\n",
    "                'min_samples_split' : [1,2,3,5]\n",
    "                }\n",
    "            \n",
    "'''\n",
    "#after a few sugestions with gridsearch e.g {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'n_estimators': 95}\n",
    "\n",
    "hyperparameters = {'n_estimators' : [96,98,100,105,120,114,135,111,95],\n",
    "                'criterion' : ['gini'],\n",
    "                'max_depth' : [None],\n",
    "                'min_samples_split' : [5]\n",
    "                }\n",
    "'''\n",
    "trainFAR, trainFRR, testFAR, testFRR = evaluate(classifierType[0], hyperparameters, features, labels,hp)\n",
    "print('Train:')\n",
    "print('\\tFAR:', trainFAR)\n",
    "print('\\tFRR:', trainFRR)\n",
    "\n",
    "print('Test:')\n",
    "print('\\tFAR:', testFAR)\n",
    "print('\\tFRR:', testFRR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   SVM: \n",
      "Train set shape: (945, 1024)\n",
      "Train labels shape: (945,)\n",
      "Test set shape: (405, 1024)\n",
      "Test labels shape: (405,)\n",
      "\tbest params are  {'C': 20, 'gamma': 0.0015, 'kernel': 'rbf'}\n",
      "Train:\n",
      "\tFAR: 0.27210884353741494\n",
      "\tFRR: 13.80952380952381\n",
      "Test:\n",
      "\tFAR: 1.9047619047619049\n",
      "\tFRR: 17.77777777777778\n"
     ]
    }
   ],
   "source": [
    "print(\"   SVM: \")\n",
    "hyperparameters=  [ \n",
    "                   {\n",
    "                      'C': [10,20,15,5,9,13,25,50, 100,500, 1000], \n",
    "                      'gamma': [0.001,0.00075,0.0009,0.0005,0.0008,0.0015,0.0017,0.00173,0.0016,0.012,0.0175,0.002,0.0025,0.0015,0.003], \n",
    "                      'kernel': ['rbf']\n",
    "                    }]\n",
    "              \n",
    "hp =    {             'C': 1, \n",
    "                      'kernel': 'linear'\n",
    "                    }\n",
    "trainFAR, trainFRR, testFAR, testFRR = evaluate(classifierType[2], hyperparameters, features, labels,hp)\n",
    "\n",
    "print('Train:')\n",
    "print('\\tFAR:', trainFAR)\n",
    "print('\\tFRR:', trainFRR)\n",
    "\n",
    "print('Test:')\n",
    "print('\\tFAR:', testFAR)\n",
    "print('\\tFRR:', testFRR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train length:  1080\ntest length:  270\n"
     ]
    }
   ],
   "source": [
    "#new data preprocessing\n",
    "#new \"split\" for new data preprocessing !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset['context'], \n",
    "                                                    labels, \n",
    "                                                    test_size=0.2,\n",
    "                                                     random_state=42\n",
    "                                                    )\n",
    "print(\"train length: \", len(X_train))\n",
    "print(\"test length: \", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " NB (CountVectorizer)\nTest:\n\tFAR: 0.9615384615384616\n\tFRR: 0.0\n general score: 0.9951923076923077\n\nTrue Positives: 62\nFalse Positives: 2\nTrue Negatives: 206\nFalse Negatives: 0\nSVC n-grams\nTest:\n\tFAR: 1.4423076923076923\n\tFRR: 6.451612903225806\n general score: 0.9605303970223326\n"
     ]
    }
   ],
   "source": [
    "print(\" NB (CountVectorizer)\")\n",
    "\n",
    "vect = CountVectorizer(ngram_range=[1,1]).fit(X_train)#it shows score 0.996 with 1-gram\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "model = MultinomialNB(alpha=0.1)\n",
    "model_fit = model.fit(X_train_vectorized, y_train)\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "testFAR, testFRR = get_metrics(y_test, predictions)\n",
    "print('Test:')\n",
    "print('\\tFAR:', testFAR)\n",
    "print('\\tFRR:', testFRR)\n",
    "aucscore = roc_auc_score(y_test, predictions)\n",
    "\n",
    "print(\" general score:\", aucscore)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "print(f'\\nTrue Positives: {tp}')\n",
    "print(f'False Positives: {fp}')\n",
    "print(f'True Negatives: {tn}')\n",
    "print(f'False Negatives: {fn}')\n",
    "\n",
    "\n",
    "\n",
    "#apperantly it worked as good as logistic regression \n",
    "\n",
    "print(\"SVC n-grams\")\n",
    "clfsvc = SVC(**{'C': 10, 'gamma': 0.002, 'kernel': 'rbf'})\n",
    "clfsvc.fit(X_train_vectorized, y_train)\n",
    "predictions = clfsvc.predict(vect.transform(X_test))\n",
    "testFAR, testFRR = get_metrics(y_test, predictions)\n",
    "print('Test:')\n",
    "print('\\tFAR:', testFAR)\n",
    "print('\\tFRR:', testFRR)\n",
    "aucscore = roc_auc_score(y_test, predictions)\n",
    "\n",
    "print(\" general score:\", aucscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " NB (TfidfVectorizer)\nTest:\n\tFAR: 0.0\n\tFRR: 14.516129032258066\n\nTrue Positives: 53\nFalse Positives: 0\nTrue Negatives: 208\nFalse Negatives: 9\n"
     ]
    }
   ],
   "source": [
    "print(\" NB (TfidfVectorizer)\")\n",
    "vect = TfidfVectorizer(min_df=2).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "model = sklearn.naive_bayes.MultinomialNB()\n",
    "model_fit = model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model_fit.predict(vect.transform(X_test))\n",
    "testFAR, testFRR = get_metrics(y_test, predictions)\n",
    "print('Test:')\n",
    "print('\\tFAR:', testFAR)\n",
    "print('\\tFRR:', testFRR)\n",
    "#proof\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "print(f'\\nTrue Positives: {tp}')\n",
    "print(f'False Positives: {fp}')\n",
    "print(f'True Negatives: {tn}')\n",
    "print(f'False Negatives: {fn}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LogisticRegression!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\nTest:\n\tFAR: 0.0\n\tFRR: 0.0\n general score: 1.0\nNB with n-grams :\nTest:\n\tFAR: 0.4807692307692308\n\tFRR: 6.451612903225806\n"
     ]
    }
   ],
   "source": [
    "#usage of the n-grams was the best intuitive idea due to the short form of masseges, adress form of text which can be \n",
    "# subjectively divided only with more then one word (click on,  you should, your bank..., the ... (talking about 3rd objects in \n",
    "# dialogue), ) but it didn't worked that good with nb. Everything got better when it's a weight-length part added to the features\n",
    "def add_feature(X, feature_to_add):   \n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')\n",
    "print(\"LogisticRegression!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "vectorizer = TfidfVectorizer(min_df=2, ngram_range=[1,2])#it shows score <0.9 with 1-gram and 0.91 with adding 3-gram\n",
    "\n",
    "X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "X_train_transformed_with_length = add_feature(X_train_transformed, [X_train.str.len(),\n",
    "                                                                    X_train.apply(lambda x: len(''.join([a for a in x if a.isdigit()])))])\n",
    "\n",
    "X_test_transformed = vectorizer.transform(X_test)\n",
    "X_test_transformed_with_length = add_feature(X_test_transformed, [X_test.str.len(),\n",
    "                                                                  X_test.apply(lambda x: len(''.join([a for a in x if a.isdigit()])))])\n",
    "\n",
    "clf = LogisticRegression(penalty = \"l2\",C=100,solver = \"lbfgs\")\n",
    "\n",
    "clf.fit(X_train_transformed_with_length, y_train)\n",
    "\n",
    "predictions = clf.predict(X_test_transformed_with_length)\n",
    "testFAR, testFRR = get_metrics(y_test, predictions)\n",
    "print('Test:')\n",
    "print('\\tFAR:', testFAR)\n",
    "print('\\tFRR:', testFRR)\n",
    "aucscore = roc_auc_score(y_test, predictions)\n",
    "\n",
    "print(\" general score:\", aucscore)\n",
    "print(\"NB with n-grams :\")\n",
    "clf_main = MultinomialNB(alpha=0.1)\n",
    "clf_main.fit(X_train_transformed_with_length, y_train)\n",
    "\n",
    "predictions = clf_main.predict(X_test_transformed_with_length)\n",
    "testFAR, testFRR = get_metrics(y_test, predictions)\n",
    "print('Test:')\n",
    "print('\\tFAR:', testFAR)\n",
    "print('\\tFRR:', testFRR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}