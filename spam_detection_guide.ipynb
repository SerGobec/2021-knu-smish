{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-candidate"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitf99a00d09c66447887f1a84f95bf111b",
   "display_name": "Python 3.8.3 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# SPAM DETECTION GUIDE\n",
    "\n",
    "This notebook presents step-by-step guide to building and training ML-algorithm for binary classification task (in this case - spam detection)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Preparation\n",
    "In this part you will set up all things"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "First of all, you need to import all necessary external modules.\n",
    "In this scenario we will use the following libraries:<br>\n",
    "tensorflow_hub  -   includes word embeddings (you also need installed tensorflow); <br>\n",
    "numpy           -   is a library used for basic math and linear algebra calculations;<br>\n",
    "pandas          -   provides a comfort way to work with large amounts of data through DataFrames;<br>\n",
    "sklearn         -   contains a lot of different algorithms and tools useful for ML projects;<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "source": [
    "Secondly, you need to specify the locations to all external files (for example, your dataset). You have to also specify all possible parameters for your code and store them in variables. This way, you will be protected from accidental typos :)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify location your dataset here\n",
    "DATA_PATH = ''\n",
    "\n",
    "# give name to label-column and text-column\n",
    "COLUMN_LABEL = ''\n",
    "COLUMN_TEXT = ''\n",
    "\n",
    "# these are labels that indicate the type of message.\n",
    "LABEL_LEGIT = 'LEGI'\n",
    "LABEL_SPAM = 'SPAM'\n",
    "LABEL_SMISHING = 'SMIS'"
   ]
  },
  {
   "source": [
    "## Dataset \n",
    "In this part, we will load dataset and check it for errors.<br>\n",
    "Here dataset is loaded from file into a DataFrame. DataFrame is basically a database or a table with columns where all the data is stored.<br>\n",
    "You can access those columns by calling them by name:<br>\n",
    "<code> column = dataframe\\[column_name\\] </code>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(DATA_PATH, sep='\\t', names=[COLUMN_LABEL, COLUMN_TEXT], header=None)\n",
    "print('Total size:', dataset.shape[0])\n",
    "print('Legit messages:', dataset[dataset[COLUMN_LABEL] == LABEL_LEGIT].shape[0])\n",
    "print('Spam messages:', dataset[dataset[COLUMN_LABEL] == LABEL_SPAM].shape[0])\n",
    "print('Smishing messages:', dataset[dataset[COLUMN_LABEL] == LABEL_SMISHING].shape[0])"
   ]
  },
  {
   "source": [
    "For now we don't realy need smishing messages, so we will remove them."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[((dataset[COLUMN_LABEL] == LABEL_LEGIT) | (dataset[COLUMN_LABEL] == LABEL_SPAM))]\n",
    "\n",
    "# Let's check if they are gone\n",
    "print('Smishing messages:', dataset[dataset[COLUMN_LABEL] == LABEL_SMISHING].shape[0])"
   ]
  },
  {
   "source": [
    "If the dataset is fine, you may proceed to next step"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Data preprocessing\n",
    "The difference between humans and machines is that the machines don't know words. So it is very difficult for them to process human language.<br>\n",
    "To help them, scientists invented a bunch of methods to translate words and sentences into sets of numbers, which machines are used to. Those methods are called \"embeddings\".<br>\n",
    "The point of embedding is to translate words (or complete sentences) into a new space of features (usualy of high dimensionality), so every word (sentence) will be represented as vector in this new space.<br>\n",
    "In this example we will use ELMO embeddings (but you can use something else if you wish).<br>\n",
    "You can also use simpler approach and transform your messages into feature-vectors using features of the message itself (for example, you can count length of every message as a feature, or you can count number of URLs).<br>\n",
    "The more different features you use, the better are chances to succeed. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages2vectors(messages):\n",
    "    '''\n",
    "    Transforms single message into feature-vector;\n",
    "    Parameters:\n",
    "        messages    -   array of strings;\n",
    "    Returns:\n",
    "        features    -   array of feature-vectors;   \n",
    "    '''\n",
    "\n",
    "    # embedding = nlu.load('elmo').predict(messages, output_level='token').elmo_embeddings\n",
    "    # features = np.matrix(embedding)\n",
    "    elmo = hub.Module(\"https://tfhub.dev/google/elmo/1\")\n",
    "\n",
    "    features = np.zeros((0, 1024))\n",
    "    n = 100\n",
    "    l = int(len(messages) / n) + 1 if len(messages) % 2 != 0 else int(len(messages) / n)\n",
    "    for i in range(l):\n",
    "        if i * n == len(messages):\n",
    "            break\n",
    "        # right = (i + 1) * n if (i + 1) * n < len(messages) else 0\n",
    "        if (i + 1) * n < len(messages):\n",
    "            right = (i + 1) * n\n",
    "            embedds = elmo(messages[int(i * n) : right], signature=\"default\", as_dict=True)[\"default\"] \n",
    "        else:\n",
    "            embedds = elmo(messages[:len(messages) - int(i * n)], signature=\"default\", as_dict=True)[\"default\"] \n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            embedds = sess.run(embedds)\n",
    "            features = np.concatenate([features, embedds])\n",
    "\n",
    "    return features"
   ]
  },
  {
   "source": [
    "Next step is to prepare labels. You need to convert all 'LEGI' labels to 0s and all 'SPAM' labels to 1s.<br>\n",
    "For example: \\['LEGI', 'LEGI', 'SPAM', 'LEGI', 'SPAM'\\] -> \\[0, 0, 1, 0, 1\\]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels(labels_raw):\n",
    "    '''\n",
    "    Transforms labels into numerical values;\n",
    "    Parameters:\n",
    "        labels_raw    -   array of text-labels;\n",
    "    Returns:\n",
    "        features    -   array of numerical labels;   \n",
    "    ''' \n",
    "\n",
    "    # add your code here\n",
    "\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "source": [
    "Now let's transform messages to features and change labels to numerical values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = messages2vectors(dataset[COLUMN_TEXT])\n",
    "labels = convert_labels(dataset[COLUMN_LABEL])\n",
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "source": [
    "To ensure that our model's predictions are valid, we need to split our data into 2 parts: training and testing.<br>\n",
    "Because of it's nature, ML-algorithms have tendency to overfit on training data (which means that algorithm no longer 'predicts' class based on input but rather remembers that a particular input corresponds to a particular output).<br>\n",
    "Overfitting causes model to completely fail on any data not present in training set.<br>\n",
    "Separate (independent from training) data provides us with unbiased results to see 'real' performance."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(features, labels, ratio=0.7):\n",
    "    '''\n",
    "    Splits dataset into train/test parts using given ratio;\n",
    "    Parameters:\n",
    "        data    -   array of features;\n",
    "        labels  -   array of corresponding labels;\n",
    "        ratio   -   train/test size ratio;\n",
    "    Returns:\n",
    "        train_data      -   array of training features;   \n",
    "        train_labels    -   array of training labels; \n",
    "        test_data       -   array of testing features; \n",
    "        test_labels     -   array of testing labels; \n",
    "    '''    \n",
    "\n",
    "\n",
    "    positive_data = features[labels == 1] # all spam features\n",
    "    negative_data = features[labels == 0] # all legit messages\n",
    "\n",
    "    # We shuffle arrays to get random samples later\n",
    "    random_indecies_positive = np.arange(positive_data.shape[0])\n",
    "    np.random.shuffle(random_indecies_positive)\n",
    "    random_indecies_negative = np.arange(negative_data.shape[0])\n",
    "    np.random.shuffle(random_indecies_negative)\n",
    "\n",
    "    n_positive_train = int(positive_data.shape[0] * ratio)\n",
    "    n_negative_train = int(negative_data.shape[0] * ratio)\n",
    "\n",
    "    # Training data are all indecies in 'ratio' part of shuffled indecies\n",
    "    train_data = np.concatenate([positive_data[random_indecies_positive[:n_positive_train]], \n",
    "                                negative_data[random_indecies_negative[:n_negative_train]]])\n",
    "    \n",
    "    train_labels = np.asarray([1] * n_positive_train + [0] * n_negative_train)\n",
    "\n",
    "    # Testing data are all indecies that remain\n",
    "    test_data = np.concatenate([positive_data[random_indecies_positive[n_positive_train:]], \n",
    "                                negative_data[random_indecies_negative[n_negative_train:]]])\n",
    "\n",
    "    test_labels = np.asarray([1] * (positive_data.shape[0]  - n_positive_train) + [0] * (negative_data.shape[0] - n_negative_train))\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Metrics\n",
    "To see how good (or bad) our spam detector works, we have to use some metrics. In assigment you are required to compare FAR and FRR of different algorithms.<br>\n",
    "FAR (False Acceptance Rate) - ratio of positive samples (spam in our case) wrongly predicted as negative (legitimate);<br>\n",
    "FRR (False Rejection Rate) - ratio of negative samples (legitimate) wrongly predicted as positive (spam);<br>\n",
    "These rates represent False Negative and False Positive Errors (you may also know them by names Type-1 Error and Type-2 Error)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Your task is to compute FAR and FRR based on given true labels and predicted labels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(labels, predictions):\n",
    "    '''\n",
    "    Computes metrics;\n",
    "    Parameters:\n",
    "        labels    -   array of labels;\n",
    "        predictions  -   array of predictions;\n",
    "    Returns:\n",
    "        FAR -   False Acceptance Rate;\n",
    "        FRR -   False Rejection Rate;\n",
    "    '''  \n",
    "    # add your code here\n",
    "    FAR =\n",
    "    FRR = \n",
    "    return FAR, FRR"
   ]
  },
  {
   "source": [
    "## Model initialization\n",
    "In this part we will create classifier and set it up. We will use Random Forest as example (note that in your assigment you must use all of given algorithms).<br>\n",
    "Note, that every algorithm has it's unique set of parameters (called hyperparameters).<br>\n",
    "Also, algorithms from <code>sklearn</code> library usualy have common methods <code>fit(X, Y)</code> and <code>predict(X)</code>."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierType = sklearn.ensemble.RandomForestClassifier\n",
    "hyperparameters = {'n_estimators' : 100,\n",
    "                'criterion' : 'gini',\n",
    "                'max_depth' : None,\n",
    "                'min_samples_split' : 2}"
   ]
  },
  {
   "source": [
    "## Model Training and evaluation\n",
    "Complete method below to do the following:<br>\n",
    "1) Split <code>data</code> an labels into training and testing sets;<br>\n",
    "2) Fit your model on training set using <code>fit(X, Y)</code> method;<br> \n",
    "3) Make predictions based on training set using <code>predict(X)</code> method;<br>\n",
    "4) Compute FAR/FRR for training set;<br>\n",
    "5) Make predictions based on testing set;<br>\n",
    "6) Compute FAR/FRR for testing set;<br>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(classifierType, hyperparameters, features, labels):\n",
    "    '''\n",
    "    Splits dataset into train/test parts using given ratio;\n",
    "    Parameters:\n",
    "        classifierType      -   type of ML algorithm to use;\n",
    "        hyperparameters     -   dictionary of model's parameters;\n",
    "        features            -   array of features;\n",
    "        labels              -   array of labels\n",
    "    Returns:\n",
    "        trainFAR    -   False Acceptance Rate for train dataset;\n",
    "        trainFRR    -   False Rejection Rate for train dataset;\n",
    "        testFAR     -   False Acceptance Rate for test dataset;\n",
    "        testFRR    -   False Rejection Rate for test dataset;\n",
    "    '''    \n",
    "\n",
    "    model = classifierType(**hyperparameters)\n",
    "\n",
    "    # Split data\n",
    "    # add your code here\n",
    "    train_data, train_labels, test_data, test_labels = \n",
    "\n",
    "    print('Train set shape:', train_data.shape)\n",
    "    print('Train labels shape:', train_labels.shape)\n",
    "    print('Test set shape:', test_data.shape)\n",
    "    print('Test labels shape:', test_labels.shape)\n",
    "\n",
    "    # Fit your model\n",
    "    # add your code here\n",
    "\n",
    "\n",
    "    # Make predictions for training dataset\n",
    "    # add your code here\n",
    "\n",
    "\n",
    "    # Compute train FAR/FRR\n",
    "    # add your code here\n",
    "    trainFAR, trainFRR = \n",
    "\n",
    "    # Make predictions for testing dataset\n",
    "    # add your code here\n",
    "    predictions_test = \n",
    "\n",
    "    # Compute test FAR/FRR\n",
    "    # add your code here\n",
    "    testFAR, testFRR = \n",
    "\n",
    "    return trainFAR, trainFRR, testFAR, testFRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if it works :)\n",
    "trainFAR, trainFRR, testFAR, testFRR = evaluate(classifierType, hyperparameters, features, labels)\n",
    "print('Train:')\n",
    "print('\\tFAR:', trainFAR)\n",
    "print('\\tFRR:', trainFRR)\n",
    "\n",
    "print('Test:')\n",
    "print('\\tFAR:', testFAR)\n",
    "print('\\tFRR:', testFRR)"
   ]
  },
  {
   "source": [
    "## Final Task\n",
    "Combine your knowledge and compute FAR/FRR metrics for other algorithms from assignment.<br>\n",
    "Don't forget to fill report and share it on google class.<br>\n",
    "Good luck :)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}